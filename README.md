# Medication Adherence Risk Model

An end-to-end ML pipeline that predicts medication non-adherence within a rolling
90-day window using synthetic pharmacy claims data. The project covers data
generation, SQL-based feature engineering, model training, evaluation, and
interpretability — structured as a portfolio project for AI/data science roles in
healthcare analytics.

---

## Why I Built This

When working with healthcare data, I noticed that adherence reporting is usually descriptive — it tells you what happened, not who is likely to fall off track next.

I wanted to build a realistic, end-to-end pipeline that moves from historical refill behavior to proactive intervention scoring. The goal wasn’t just high ROC-AUC, but understanding which behavioral signals truly drive non-adherence and how threshold decisions affect operational tradeoffs (e.g., false negatives vs intervention cost).

This project simulates what a lightweight early-warning system could look like in practice — from feature engineering to model evaluation and interpretability.

---

## What this does

Medication non-adherence costs the US healthcare system an estimated $300B+ annually
and is one of the few problems where a well-timed intervention (a pharmacist call,
a refill reminder) can close the gap before it becomes a clinical event.

This pipeline simulates what that early-warning system looks like in practice:
generate a patient population from synthetic claims data, engineer adherence metrics
(PDC, MPR, refill gaps), train a classifier to flag patients at risk of lapsing
in the next 90 days, and evaluate where the model is right and where it isn't.

The feature engineering logic is written twice — once in SQL
(`sql/feature_engineering.sql`) to show how it would run in a production data
warehouse, and once in Python (`src/feature_engineering.py`) so the full pipeline
runs locally without a database.

---


## Repository structure

```
medication-adherence-risk-model/
├── README.md
├── requirements.txt
├── .gitignore
├── run_all.sh                    # Single command to run full pipeline
├── data/
│   ├── raw/                      # Generated by data_generate.py
│   │   ├── patients.csv
│   │   ├── prescriptions.csv
│   │   └── refills.csv
│   └── processed/                # Generated by feature_engineering.py + data_prep.py
│       ├── features.csv
│       ├── train.csv
│       ├── test.csv
│       └── models/               # Serialized model pipelines (.pkl)
├── sql/
│   └── feature_engineering.sql   # PDC/MPR/gap logic in SQL (demonstration)
├── src/
│   ├── __init__.py
│   ├── config.py                 # All paths, seeds, and parameters
│   ├── utils.py
│   ├── data_generate.py          # Synthetic data generation
│   ├── feature_engineering.py    # PDC, MPR, gap features
│   ├── data_prep.py              # Train/test split
│   ├── train.py                  # Logistic Regression + Random Forest
│   └── evaluate.py               # Metrics + all figures
└── reports/
    ├── metrics.json
    ├── model_card.md
    ├── short_report.md
    └── figures/
        ├── roc_curve.png
        ├── confusion_matrix.png
        ├── feature_importance.png
        └── score_distribution.png
```

---

## Setup

**Requirements:** Python 3.10+

```bash
git clone https://github.com/YOUR_USERNAME/medication-adherence-risk-model
cd medication-adherence-risk-model
pip install -r requirements.txt
```

---

## Running the pipeline

### Option 1 — Single command

```bash
bash run_all.sh
```

### Option 2 — Step by step

```bash
# 1. Generate synthetic patients, prescriptions, and refills
python -m src.data_generate

# 2. Engineer features (PDC, MPR, refill gaps)
python -m src.feature_engineering

# 3. Clean data and create train/test split
python -m src.data_prep

# 4. Train Logistic Regression + Random Forest
python -m src.train

# 5. Evaluate and generate all figures
python -m src.evaluate
```

---

## Features explained

| Feature | What it captures |
|---------|-----------------|
| `prior_pdc` | Proportion of Days Covered in prior 180-day window. PDC < 0.80 is the standard non-adherence threshold. |
| `prior_mpr` | Medication Possession Ratio. Total supply dispensed / window days, capped at 1.0. |
| `avg_refill_gap_days` | How many days, on average, elapsed between consecutive fills. |
| `max_refill_gap_days` | The longest single gap in refill history — captures one-off disruptions. |
| `refill_gap_std` | Variability in gap lengths. High variance = unpredictable refill behavior. |
| `overlap_days` | Total days of early fills (patient picking up before supply runs out). |
| `switch_count` | Number of drug class switches — proxy for treatment instability. |
| `chronic_flag` | Whether patient has a chronic condition. Chronic patients tend to be more adherent. |
| `num_drug_classes` | Count of distinct drug classes prescribed — medication burden. |

**PDC vs. MPR:** PDC counts unique covered days (no double-counting for early fills).
MPR counts total days supplied and can exceed 1.0. In practice, PDC is the preferred
metric in CMS Star Ratings programs because it's more conservative.

---

## Results

| Model | ROC-AUC | F1 | Recall | Precision |
|-------|---------|-----|--------|-----------|
| Logistic Regression | **0.976** | 0.890 | 0.894 | 0.886 |
| Random Forest | 0.979 | 0.879 | 0.911 | 0.849 |

The logistic regression is the recommended model — it's within 0.003 AUC of the
Random Forest while being fully interpretable. The coefficients confirm that
`avg_refill_gap_days` and `prior_pdc` are the dominant predictors, which matches
clinical intuition.

**Figures:**
- `reports/figures/roc_curve.png` — ROC curves for both models
- `reports/figures/confusion_matrix.png` — Confusion matrix (Logistic Regression)
- `reports/figures/feature_importance.png` — LR coefficients + RF importances side-by-side
- `reports/figures/score_distribution.png` — Risk score distributions by class

---

## What I'd do next

**Calibration.** Predicted probabilities aren't calibrated out of the box. Platt
scaling would let you set thresholds based on actual probability estimates rather
than raw scores.

**Threshold optimization.** In a real deployment, false negatives (missing a
patient who lapses) likely cost more than false positives (unnecessary outreach).
The decision threshold should be set based on that cost ratio, not defaulted to 0.5.

**Fairness evaluation.** Before putting this in front of care coordinators, I'd
want recall broken down by age group, chronic condition type, and any available
demographic proxies. Differential performance is common in health models.

**Richer features.** Real PBM/EHR data would include diagnosis codes, lab values,
ER utilization, and insurance type — all of which have documented relationships
with adherence. Current features are pharmacy-data-only.

**Time-series component.** A rolling lag feature (is the patient's gap trend
worsening?) would capture trajectory, not just snapshot. Patients heading in the
wrong direction are a different risk profile than patients with a stable history
of borderline adherence.

---

## Limitations & Design Tradeoffs

Early iterations suffered from feature leakage when refill gap features were constructed too close to the prediction window, artificially inflating performance. Feature windows were adjusted to better simulate real-world forecasting conditions.

While Random Forest slightly outperformed Logistic Regression on ROC-AUC, Logistic Regression offered stronger interpretability, an important consideration in healthcare contexts where stakeholders need explainable risk drivers.

The dataset is synthetic. Although designed to resemble real pharmacy claims patterns, calibration behavior and feature distributions may differ from real-world adherence data, where noise and behavioral variability are higher.

---

## Notes

All data is **fully synthetic** — no real patient data, PHI, or de-identified
records were used at any stage. See `reports/model_card.md` for full documentation
of intended use, limitations, and ethical considerations.
